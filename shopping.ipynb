{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e2fded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "TEST_SIZE = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df53ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "    Load the shopping CSV and apply simple preprocessing.\n",
    "\n",
    "    - Reads `filename` into a DataFrame.\n",
    "    - Encodes the `Month` column with LabelEncoder.\n",
    "    - Converts `VisitorType` to a binary flag (Returning_Visitor -> 1 else 0).\n",
    "    - Ensures `Weekend` is an integer (0/1).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    evidence : pandas.DataFrame\n",
    "        Feature matrix (all columns except the `Revenue` label).\n",
    "    labels : pandas.Series\n",
    "        Binary labels (Revenue converted to int).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    df['Month'] = encoder.fit_transform(df['Month'])\n",
    "    df['VisitorType'] = (df['VisitorType'] == 'Returning_Visitor').astype(int)\n",
    "    df['Weekend'] = df['Weekend'].astype(int)\n",
    "\n",
    "    labels = df['Revenue'].astype(int)\n",
    "    evidence = df.drop('Revenue', axis=1)\n",
    "\n",
    "    return evidence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "448e1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_classes(evidence, labels, alpha=0.25):\n",
    "    \"\"\"\n",
    "    Balance the classes by oversampling the minority class.\n",
    "\n",
    "    Notes on `alpha` (no behavior changes performed here):\n",
    "    - Implementation oversamples the minority class to a target size computed as\n",
    "      round((1 - alpha) * len(majority_class)).\n",
    "    - alpha = 0.0 -> minority is oversampled up to the full majority size (fully balanced)\n",
    "    - alpha > 0.0 -> minority is oversampled to a fraction of the majority size\n",
    "      (e.g. alpha=0.25 -> minority oversampled to 75% of majority size)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    evidence : pandas.DataFrame\n",
    "        Feature matrix.\n",
    "    labels : pandas.Series or array-like\n",
    "        Binary labels (0/1).\n",
    "    alpha : float, optional\n",
    "        Controls the target oversample size (see notes above). Default 0.25.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    balanced_evidence, balanced_labels\n",
    "        DataFrame and Series with classes re-balanced according to `alpha`.\n",
    "    \"\"\"\n",
    "    # Combine evidence and labels into a single DataFrame\n",
    "    data = evidence.copy()\n",
    "    data['Label'] = labels\n",
    "\n",
    "    # Separate majority and minority classes\n",
    "    majority_class = data[data['Label'] == 0]\n",
    "    minority_class = data[data['Label'] == 1]\n",
    "\n",
    "    # Undersample majority class (old approach - now using oversampling)\n",
    "    # Oversample minority class to the target size computed from alpha\n",
    "    minority_class_oversampled = minority_class.sample(n=round((1-alpha)*len(majority_class)), replace=True, random_state=42)\n",
    "\n",
    "    # Combine majority class with oversampled minority class\n",
    "    balanced_data = pd.concat([majority_class, minority_class_oversampled])\n",
    "\n",
    "    # Shuffle the balanced dataset\n",
    "    balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Separate evidence and labels\n",
    "    balanced_labels = balanced_data['Label']\n",
    "    balanced_evidence = balanced_data.drop('Label', axis=1)\n",
    "\n",
    "    return balanced_evidence, balanced_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eda980cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data():\n",
    "    \"\"\"\n",
    "    Create and return a ColumnTransformer preprocessor.\n",
    "\n",
    "    - Currently applies StandardScaler to a set of numeric duration / count features.\n",
    "    - Keeps the transformer separate so it can be reused in a Pipeline.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    preprocessor : sklearn.compose.ColumnTransformer\n",
    "        Transformer that scales numeric columns.\n",
    "    \"\"\"\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('numeric', StandardScaler(), [ 'Administrative_Duration',\n",
    "                                            'Informational_Duration',\n",
    "                                            'ProductRelated',\n",
    "                                            'ProductRelated_Duration',\n",
    "                                            'PageValues'\n",
    "                                                                                    ])\n",
    "\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f2f76b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipline():\n",
    "    \"\"\"\n",
    "    Build and return two training pipelines:\n",
    "      - KNN pipeline: preprocessing -> KNeighborsClassifier\n",
    "      - LogisticRegression pipeline: preprocessing -> LogisticRegression\n",
    "\n",
    "    Note: function name is `pipline` (typo) â€” keep it as-is to avoid breaking existing calls.\n",
    "    Returns\n",
    "    -------\n",
    "    (knn_pipe, logreg_pipe) : tuple\n",
    "        Two sklearn Pipeline objects ready for GridSearchCV or fitting.\n",
    "    \"\"\"\n",
    "    preprocessor = preprocess_data()\n",
    "    knn_pipe = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', KNeighborsClassifier()),\n",
    "    ])\n",
    "\n",
    "    # Add class_weight='balanced' to handle imbalanced data\n",
    "    logreg_pipe = Pipeline(\n",
    "        steps=[\n",
    "            ('preprocessor', preprocessor), \n",
    "\n",
    "            # it changes the decision boundary to account for class imbalance\n",
    "            # penalize mistakes on the minority class more heavily\n",
    "            ('logistic', LogisticRegression(class_weight='balanced', max_iter=1000))\n",
    "        ]\n",
    "    )\n",
    "    return knn_pipe, logreg_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "899a83b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_with_gridsearch(knn_pipe: Pipeline, logreg_pipe: Pipeline, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Prepare GridSearchCV objects for KNN and Logistic Regression.\n",
    "\n",
    "    This function returns configured (but not fitted) GridSearchCV objects that\n",
    "    can be fit on different training sets. Returning the objects this way lets\n",
    "    callers inspect `cv_results_` after fitting.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    knn_pipe : sklearn.pipeline.Pipeline\n",
    "        Pipeline containing preprocessor and a KNeighborsClassifier placeholder.\n",
    "    logreg_pipe : sklearn.pipeline.Pipeline\n",
    "        Pipeline containing preprocessor and a LogisticRegression placeholder.\n",
    "    X_train, y_train : used for compatibility (not used to fit here)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grid_search_knn, grid_search_logreg : tuple\n",
    "        Configured GridSearchCV objects (not fitted).\n",
    "    \"\"\"\n",
    "    param_grid_knn = {\n",
    "        'classifier__n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "        'classifier__weights': ['uniform', 'distance'],\n",
    "        'classifier__metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "    }\n",
    "    param_grid_logreg = {\n",
    "        'logistic__C': [0.01, 0.1, 1, 10, 100],\n",
    "    }\n",
    "    \n",
    "    grid_search_knn = GridSearchCV(knn_pipe, param_grid_knn, cv=5, scoring='balanced_accuracy')\n",
    "    grid_search_logreg = GridSearchCV(logreg_pipe, param_grid_logreg, cv=5, scoring='balanced_accuracy')\n",
    "\n",
    "    return grid_search_knn, grid_search_logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eed88b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(evidence, labels):\n",
    "    \"\"\"\n",
    "    Train models using GridSearch over KNN hyperparameters and alpha-based balancing.\n",
    "\n",
    "    Process:\n",
    "    - For each alpha value, the training data is rebalanced via `balance_classes`.\n",
    "    - GridSearchCV is run for the KNN pipeline on the balanced data.\n",
    "    - The best performing alpha (by CV score) is reported.\n",
    "    - A separate GridSearchCV is fit for LogisticRegression on the original data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_knn_model, best_logreg_model\n",
    "        The best estimators (from the GridSearchCV runs) for KNN and LogisticRegression.\n",
    "    \"\"\"\n",
    "    knn_pipe, logreg_pipe = pipline()\n",
    "    \n",
    "    # Define parameter grid including alpha values for balancing\n",
    "    param_grid_knn = {\n",
    "        'classifier__n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "        'classifier__weights': ['uniform', 'distance'],\n",
    "        'classifier__metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "    }\n",
    "    param_grid_logreg = {\n",
    "        'logistic__C': [0.01, 0.1, 1, 10, 100],\n",
    "    }\n",
    "    \n",
    "    # Alpha values to try for balancing (0 = fully balanced, higher = more minority bias)\n",
    "    alpha_values = [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "    \n",
    "    best_score = -1\n",
    "    best_alpha = None\n",
    "    best_knn_model = None\n",
    "    \n",
    "    # Try each alpha value\n",
    "    for alpha in alpha_values:\n",
    "        # Rebalance training data for this alpha\n",
    "        balanced_evidence, balanced_labels = balance_classes(evidence, labels, alpha=alpha)\n",
    "\n",
    "        # Grid search for KNN on the balanced data\n",
    "        grid_search_knn = GridSearchCV(knn_pipe, param_grid_knn, cv=5, scoring='balanced_accuracy')\n",
    "        grid_search_knn.fit(balanced_evidence, balanced_labels)\n",
    "        \n",
    "        if grid_search_knn.best_score_ > best_score:\n",
    "            best_score = grid_search_knn.best_score_\n",
    "            best_alpha = alpha\n",
    "            best_knn_model = grid_search_knn.best_estimator_\n",
    "    \n",
    "    print(f\"Best alpha for KNN: {best_alpha} (CV score: {best_score:.4f})\")\n",
    "    \n",
    "    # Train logistic regression without balancing\n",
    "    grid_search_logreg = GridSearchCV(logreg_pipe, param_grid_logreg, cv=5, scoring='balanced_accuracy')\n",
    "    grid_search_logreg.fit(evidence, labels)\n",
    "    \n",
    "    return best_knn_model, grid_search_logreg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bee78347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(labels, predictions):\n",
    "    \"\"\"\n",
    "    Compute sensitivity (true positive rate) and specificity (true negative rate).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : array-like\n",
    "        Ground-truth binary labels (0/1).\n",
    "    predictions : array-like\n",
    "        Predicted binary labels (0/1).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (sensitivity, specificity) : tuple of floats\n",
    "        sensitivity = TP / (TP + FN)\n",
    "        specificity = TN / (TN + FP)\n",
    "    \"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, predictions).ravel()\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    return sensitivity, specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cc2a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    High-level runner:\n",
    "    - Loads data\n",
    "    - Splits train / test\n",
    "    - Trains models (KNN with alpha tuning, LogisticRegression)\n",
    "    - Evaluates on the test set and prints summary metrics\n",
    "\n",
    "    Note: main currently prints results and does not persist models to disk.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load data from spreadsheet and split into train and test sets\n",
    "    evidence, labels = load_data(\"shopping.csv\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        evidence, labels, test_size=TEST_SIZE, random_state=420, stratify=labels\n",
    "    )\n",
    "\n",
    "    # Train model and make predictions\n",
    "    knn_model, logreg_model= train_model(X_train, y_train)\n",
    "    predictions_knn = knn_model.predict(X_test) # type: ignore\n",
    "    predictions_logreg = logreg_model.predict(X_test) \n",
    "\n",
    "      \n",
    "    sensitivity, specificity = evaluate(y_test, predictions_knn)\n",
    "    sensitivity_logreg, specificity_logreg = evaluate(y_test, predictions_logreg)\n",
    "\n",
    "    print(\"K-Nearest Neighbors Results:\")\n",
    "    print(f\"Correct: {(y_test == predictions_knn).sum()}\")\n",
    "    print(f\"Incorrect: {(y_test != predictions_knn).sum()}\")\n",
    "    print(f\"True Positive Rate: {100 * sensitivity:.2f}%\")\n",
    "    print(f\"True Negative Rate: {100 * specificity:.2f}%\")\n",
    "\n",
    "\n",
    "    print(\"\\nLogistic Regression Results:\")\n",
    "    print(f\"Correct: {(y_test == predictions_logreg).sum()}\")\n",
    "    print(f\"Incorrect: {(y_test != predictions_logreg).sum()}\")\n",
    "    print(f\"True Positive Rate: {100 * sensitivity_logreg:.2f}%\")\n",
    "    print(f\"True Negative Rate: {100 * specificity_logreg:.2f}%\")\n",
    "\n",
    "    # Return nothing (prints summary). If you want the fitted models or metrics,\n",
    "    # modify this function to return them for downstream visualization.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f329055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha for KNN: 0.3 (CV score: 0.9274)\n",
      "K-Nearest Neighbors Results:\n",
      "Correct: 4190\n",
      "Incorrect: 742\n",
      "True Positive Rate: 72.35%\n",
      "True Negative Rate: 87.26%\n",
      "\n",
      "Logistic Regression Results:\n",
      "Correct: 4316\n",
      "Incorrect: 616\n",
      "True Positive Rate: 72.48%\n",
      "True Negative Rate: 90.26%\n",
      "K-Nearest Neighbors Results:\n",
      "Correct: 4190\n",
      "Incorrect: 742\n",
      "True Positive Rate: 72.35%\n",
      "True Negative Rate: 87.26%\n",
      "\n",
      "Logistic Regression Results:\n",
      "Correct: 4316\n",
      "Incorrect: 616\n",
      "True Positive Rate: 72.48%\n",
      "True Negative Rate: 90.26%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b622b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
